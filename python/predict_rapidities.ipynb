{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from IPython.display import SVG\n",
    "import IPython\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import RMSprop\n",
    "\n",
    "import lieb_liniger_state as lls\n",
    "\n",
    "np.random.seed(7)\n",
    "\n",
    "# Necessary since otherwise we can't compare the accuracy between naive and ML initial guesses.\n",
    "keras.backend.set_floatx('float64')\n",
    "\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_of_particles = 5\n",
    "no_of_states = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_state = lls.lieb_liniger_state(1, no_of_particles, no_of_particles)\n",
    "Is = np.zeros((no_of_states, no_of_particles))\n",
    "lambdas = np.zeros((no_of_states, no_of_particles))\n",
    "for i in range(no_of_states):\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Generated {i}/{no_of_states} states\")\n",
    "    bethe_numbers = lls.generate_bethe_numbers(no_of_particles, ground_state.Is)\n",
    "    llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "    llstate.lambdas = 2 * np.pi / llstate.L * llstate.Is\n",
    "    no_of_iterations = llstate.calculate_rapidities_newton()\n",
    "    Is[i] = bethe_numbers\n",
    "    lambdas[i] = llstate.lambdas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neural_net(no_of_particles):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(units=no_of_particles, kernel_initializer='lecun_uniform', activation='tanh', input_dim=no_of_particles))\n",
    "#     model.add(Dense(units=no_of_particles**2, kernel_initializer='lecun_uniform', activation='tanh'))\n",
    "    model.add(Dense(units=no_of_particles**3, kernel_initializer='lecun_uniform', activation='tanh'))\n",
    "#     model.add(Dense(units=no_of_particles**2, kernel_initializer='lecun_uniform', activation='tanh'))\n",
    "    model.add(Dense(units=no_of_particles, kernel_initializer='lecun_uniform'))\n",
    "    model.compile(loss='mse', optimizer=RMSprop())\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs=100\n",
    "model = neural_net(no_of_particles)\n",
    "keras.utils.plot_model(model, to_file='test_keras_plot_model.png', show_shapes=True)\n",
    "# IPython.display.Image('test_keras_plot_model.png')\n",
    "\n",
    "history = model.fit(x=Is, y=lambdas, epochs=epochs, verbose=1, validation_split=0.05, \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', min_delta=0.0002, patience=5, verbose=1, mode='auto')]\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, len(history.history[\"val_loss\"]) + 1), history.history[\"val_loss\"])\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Validation error\")\n",
    "plt.show()\n",
    "\n",
    "print(model.predict(Is[0].reshape(1, -1), batch_size=1)[0])\n",
    "print(Is[0])\n",
    "print(lambdas[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nstates = 1000\n",
    "\n",
    "dp = pd.DataFrame(data=np.zeros(shape=(6000, 3)), columns=[\"Iterations\", \"Method\", \"Damped\"])\n",
    "\n",
    "for k in range(nstates):\n",
    "    bethe_numbers = lls.generate_bethe_numbers(no_of_particles, ground_state.Is)\n",
    "\n",
    "    # Naive Bethe (damped)\n",
    "    llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "    llstate.lambdas = 2 * np.pi / llstate.L * llstate.Is\n",
    "    no_of_iterations = llstate.calculate_rapidities_newton(enable_damping=True, printing=False)\n",
    "    dp.iloc[k] = [no_of_iterations, \"Naive Bethe\", True]\n",
    "    \n",
    "    # Naive Bethe (undamped)\n",
    "    llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "    llstate.lambdas = 2 * np.pi / llstate.L * llstate.Is\n",
    "    no_of_iterations = llstate.calculate_rapidities_newton(enable_damping=False, printing=False)\n",
    "    dp.iloc[k+nstates] = [no_of_iterations, \"Naive Bethe\", False]\n",
    "\n",
    "    # Machine learning (damped)\n",
    "    llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "    llstate.lambdas = model.predict(bethe_numbers.reshape(1, -1), batch_size=1)[0]\n",
    "    no_of_iterations = llstate.calculate_rapidities_newton(enable_damping=True, printing=False)\n",
    "    dp.iloc[k+2*nstates] = [no_of_iterations, \"ML\", True]\n",
    "    \n",
    "    # Machine learning (undamped)\n",
    "    llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "    llstate.lambdas = model.predict(bethe_numbers.reshape(1, -1), batch_size=1)[0]\n",
    "    no_of_iterations = llstate.calculate_rapidities_newton(enable_damping=False, printing=False)\n",
    "    dp.iloc[k+3*nstates] = [no_of_iterations, \"ML\", False]\n",
    "\n",
    "\n",
    "    # Random (damped)\n",
    "    llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "    llstate.lambdas = np.random.normal(0, no_of_particles, no_of_particles)\n",
    "    no_of_iterations = llstate.calculate_rapidities_newton(enable_damping=True, printing=False)\n",
    "    dp.iloc[k+4*nstates] = [no_of_iterations, \"Random\", True]\n",
    "    \n",
    "    # Random (undamped)\n",
    "    llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "    llstate.lambdas = np.random.normal(0, no_of_particles, no_of_particles)\n",
    "    no_of_iterations = llstate.calculate_rapidities_newton(enable_damping=False, printing=False)\n",
    "    dp.iloc[k+5*nstates] = [no_of_iterations, \"Random\", False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"ticks\")\n",
    "ax = sns.barplot(x=\"Method\", y=\"Iterations\", hue=\"Damped\", data=dp, ci=\"sd\")\n",
    "sns.despine()\n",
    "       \n",
    "print(dp.loc[(dp['Method'] == \"Naive Bethe\") & dp['Damped']][\"Iterations\"].mean())\n",
    "print(dp.loc[(dp['Method'] == \"Naive Bethe\") & dp['Damped']][\"Iterations\"].std())\n",
    "\n",
    "print(dp.loc[(dp['Method'] == \"Naive Bethe\") & ~dp['Damped']][\"Iterations\"].mean())\n",
    "print(dp.loc[(dp['Method'] == \"Naive Bethe\") & ~dp['Damped']][\"Iterations\"].std(), \"\\n\")\n",
    "\n",
    "print(dp.loc[(dp['Method'] == \"ML\") & dp['Damped']][\"Iterations\"].mean())\n",
    "print(dp.loc[(dp['Method'] == \"ML\") & dp['Damped']][\"Iterations\"].std())\n",
    "\n",
    "print(dp.loc[(dp['Method'] == \"ML\") & ~dp['Damped']][\"Iterations\"].mean())\n",
    "print(dp.loc[(dp['Method'] == \"ML\") & ~dp['Damped']][\"Iterations\"].std(), \"\\n\")\n",
    "\n",
    "print(dp.loc[(dp['Method'] == \"Random\") & dp['Damped']][\"Iterations\"].mean())\n",
    "print(dp.loc[(dp['Method'] == \"Random\") & dp['Damped']][\"Iterations\"].std())\n",
    "\n",
    "print(dp.loc[(dp['Method'] == \"Random\") & ~dp['Damped']][\"Iterations\"].mean())\n",
    "print(dp.loc[(dp['Method'] == \"Random\") & ~dp['Damped']][\"Iterations\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# times = pd.DataFrame(data=np.zeros(shape=(30, 3)), columns=[\"Times\", \"Method\", \"Damped\"])\n",
    "\n",
    "# # Naive Bethe (undamped)\n",
    "# for t in range(10):\n",
    "#     start_time = time.time()\n",
    "#     for k in range(1000):\n",
    "#         bethe_numbers = lls.generate_bethe_numbers(no_of_particles, ground_state.Is)\n",
    "\n",
    "#         llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "#         llstate.lambdas = 2 * np.pi / llstate.L * llstate.Is\n",
    "#         no_of_iterations = llstate.calculate_rapidities_newton(enable_damping=False, printing=False)\n",
    "#     times.iloc[t] = [time.time() - start_time, \"Naive Bethe\", \"False\"]\n",
    "\n",
    "# # Machine learning (undamped)\n",
    "# for t in range(10):\n",
    "#     start_time = time.time()\n",
    "#     for k in range(1000):\n",
    "#         bethe_numbers = lls.generate_bethe_numbers(no_of_particles, ground_state.Is)\n",
    "\n",
    "#         llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "#         llstate.lambdas = model.predict(bethe_numbers.reshape(1, -1), batch_size=1)[0]\n",
    "#         no_of_iterations = llstate.calculate_rapidities_newton(enable_damping=False, printing=False)\n",
    "#     times.iloc[t+10] = [time.time() - start_time, \"ML\", \"False\"]\n",
    "    \n",
    "# # Random (undamped)\n",
    "# for t in range(10):\n",
    "#     start_time = time.time()\n",
    "#     for k in range(1000):\n",
    "#         bethe_numbers = lls.generate_bethe_numbers(no_of_particles, ground_state.Is)\n",
    "\n",
    "#         llstate = lls.lieb_liniger_state(1, no_of_particles, no_of_particles, bethe_numbers)\n",
    "#         llstate.lambdas = np.random.normal(0, no_of_particles, no_of_particles)\n",
    "#         no_of_iterations = llstate.calculate_rapidities_newton(enable_damping=False, printing=False)\n",
    "#     times.iloc[t+20] = [time.time() - start_time, \"Random\", \"False\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.set_style(\"ticks\")\n",
    "# ax = sns.barplot(x=\"Method\", y=\"Times\", data=times, ci=\"sd\")\n",
    "# sns.despine()\n",
    "\n",
    "# print(times.loc[(times['Method'] == \"ML\")][\"Times\"].mean())\n",
    "# print(times.loc[(times['Method'] == \"ML\")][\"Times\"].std(), \"\\n\")\n",
    "\n",
    "# print(times.loc[(times['Method'] == \"Naive Bethe\")][\"Times\"].mean())\n",
    "# print(times.loc[(times['Method'] == \"Naive Bethe\")][\"Times\"].std(), \"\\n\")\n",
    "\n",
    "# print(times.loc[(times['Method'] == \"Random\")][\"Times\"].mean())\n",
    "# print(times.loc[(times['Method'] == \"Random\")][\"Times\"].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
